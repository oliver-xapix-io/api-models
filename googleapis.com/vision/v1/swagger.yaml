swagger: '2.0'
schemes:
  - https
host: vision.googleapis.com
basePath: /
info:
  contact:
    name: Google
    url: 'https://google.com'
  description: 'Integrates Google Vision features, including image labeling, face, logo, and landmark detection, optical character recognition (OCR), and detection of explicit content, into applications.'
  title: Google Cloud Vision
  version: v1
  x-apiClientRegistration:
    url: 'https://console.developers.google.com'
  x-logo:
    url: 'https://apis-guru.github.io/api-models/cache/googleapis.com/vision/v1/logo.png'
  x-origin:
    format: google
    url: 'https://vision.googleapis.com/$discovery/rest?version=v1'
    version: v1
  x-preferred: true
  x-providerName: googleapis.com
  x-serviceName: vision
externalDocs:
  url: 'https://cloud.google.com/vision/'
securityDefinitions:
  Oauth2:
    authorizationUrl: 'https://accounts.google.com/o/oauth2/auth'
    description: Oauth 2.0 authentication
    flow: implicit
    scopes:
      'https://www.googleapis.com/auth/cloud-platform': View and manage your data across Google Cloud Platform services
    type: oauth2
parameters:
  $.xgafv:
    description: V1 error format.
    enum:
      - '1'
      - '2'
    in: query
    name: $.xgafv
    type: string
  access_token:
    description: OAuth access token.
    in: query
    name: access_token
    type: string
  alt:
    default: json
    description: Data format for response.
    enum:
      - json
      - media
      - proto
    in: query
    name: alt
    type: string
  bearer_token:
    description: OAuth bearer token.
    in: query
    name: bearer_token
    type: string
  callback:
    description: JSONP
    in: query
    name: callback
    type: string
  fields:
    description: Selector specifying which fields to include in a partial response.
    in: query
    name: fields
    type: string
  key:
    description: 'API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token.'
    in: query
    name: key
    type: string
  oauth_token:
    description: OAuth 2.0 token for the current user.
    in: query
    name: oauth_token
    type: string
  pp:
    default: true
    description: Pretty-print response.
    in: query
    name: pp
    type: boolean
  prettyPrint:
    default: true
    description: Returns response with indentations and line breaks.
    in: query
    name: prettyPrint
    type: boolean
  quotaUser:
    description: 'Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters.'
    in: query
    name: quotaUser
    type: string
  uploadType:
    description: 'Legacy upload protocol for media (e.g. "media", "multipart").'
    in: query
    name: uploadType
    type: string
  upload_protocol:
    description: 'Upload protocol for media (e.g. "raw", "multipart").'
    in: query
    name: upload_protocol
    type: string
tags:
  - name: images
paths:
  '/v1/images:annotate':
    parameters:
      - $ref: '#/parameters/access_token'
      - $ref: '#/parameters/prettyPrint'
      - $ref: '#/parameters/key'
      - $ref: '#/parameters/quotaUser'
      - $ref: '#/parameters/pp'
      - $ref: '#/parameters/fields'
      - $ref: '#/parameters/alt'
      - $ref: '#/parameters/$.xgafv'
      - $ref: '#/parameters/callback'
      - $ref: '#/parameters/uploadType'
      - $ref: '#/parameters/oauth_token'
      - $ref: '#/parameters/upload_protocol'
      - $ref: '#/parameters/bearer_token'
    post:
      description: Run image detection and annotation for a batch of images.
      operationId: vision.images.annotate
      parameters:
        - in: body
          name: body
          schema:
            $ref: '#/definitions/BatchAnnotateImagesRequest'
      responses:
        '200':
          description: Successful response
          schema:
            $ref: '#/definitions/BatchAnnotateImagesResponse'
      security:
        - Oauth2:
            - 'https://www.googleapis.com/auth/cloud-platform'
      tags:
        - images
definitions:
  AnnotateImageRequest:
    description: |-
      Request for performing Google Cloud Vision API tasks over a user-provided
      image, with user-requested features.
    properties:
      features:
        description: Requested features.
        items:
          $ref: '#/definitions/Feature'
        type: array
      image:
        $ref: '#/definitions/Image'
        description: The image to be processed.
      imageContext:
        $ref: '#/definitions/ImageContext'
        description: Additional context that may accompany the image.
    type: object
  AnnotateImageResponse:
    description: Response to an image annotation request.
    properties:
      error:
        $ref: '#/definitions/Status'
        description: |-
          If set, represents the error message for the operation.
          Note that filled-in mage annotations are guaranteed to be
          correct, even when <code>error</code> is non-empty.
      faceAnnotations:
        description: 'If present, face detection completed successfully.'
        items:
          $ref: '#/definitions/FaceAnnotation'
        type: array
      imagePropertiesAnnotation:
        $ref: '#/definitions/ImageProperties'
        description: 'If present, image properties were extracted successfully.'
      labelAnnotations:
        description: 'If present, label detection completed successfully.'
        items:
          $ref: '#/definitions/EntityAnnotation'
        type: array
      landmarkAnnotations:
        description: 'If present, landmark detection completed successfully.'
        items:
          $ref: '#/definitions/EntityAnnotation'
        type: array
      logoAnnotations:
        description: 'If present, logo detection completed successfully.'
        items:
          $ref: '#/definitions/EntityAnnotation'
        type: array
      safeSearchAnnotation:
        $ref: '#/definitions/SafeSearchAnnotation'
        description: 'If present, safe-search annotation completed successfully.'
      textAnnotations:
        description: 'If present, text (OCR) detection completed successfully.'
        items:
          $ref: '#/definitions/EntityAnnotation'
        type: array
    type: object
  BatchAnnotateImagesRequest:
    description: Multiple image annotation requests are batched into a single service call.
    properties:
      requests:
        description: Individual image annotation requests for this batch.
        items:
          $ref: '#/definitions/AnnotateImageRequest'
        type: array
    type: object
  BatchAnnotateImagesResponse:
    description: Response to a batch image annotation request.
    properties:
      responses:
        description: Individual responses to image annotation requests within the batch.
        items:
          $ref: '#/definitions/AnnotateImageResponse'
        type: array
    type: object
  BoundingPoly:
    description: A bounding polygon for the detected image annotation.
    properties:
      vertices:
        description: The bounding polygon vertices.
        items:
          $ref: '#/definitions/Vertex'
        type: array
    type: object
  Color:
    description: |-
      Represents a color in the RGBA color space. This representation is designed
      for simplicity of conversion to/from color representations in various
      languages over compactness; for example, the fields of this representation
      can be trivially provided to the constructor of "java.awt.Color" in Java; it
      can also be trivially provided to UIColor's "+colorWithRed:green:blue:alpha"
      method in iOS; and, with just a little work, it can be easily formatted into
      a CSS "rgba()" string in JavaScript, as well. Here are some examples:

      Example (Java):

           import com.google.type.Color;

           // ...
           public static java.awt.Color fromProto(Color protocolor) {
             float alpha = protocolor.hasAlpha()
                 ? protocolor.getAlpha().getValue()
                 : 1.0;

             return new java.awt.Color(
                 protocolor.getRed(),
                 protocolor.getGreen(),
                 protocolor.getBlue(),
                 alpha);
           }

           public static Color toProto(java.awt.Color color) {
             float red = (float) color.getRed();
             float green = (float) color.getGreen();
             float blue = (float) color.getBlue();
             float denominator = 255.0;
             Color.Builder resultBuilder =
                 Color
                     .newBuilder()
                     .setRed(red / denominator)
                     .setGreen(green / denominator)
                     .setBlue(blue / denominator);
             int alpha = color.getAlpha();
             if (alpha != 255) {
               result.setAlpha(
                   FloatValue
                       .newBuilder()
                       .setValue(((float) alpha) / denominator)
                       .build());
             }
             return resultBuilder.build();
           }
           // ...

      Example (iOS / Obj-C):

           // ...
           static UIColor* fromProto(Color* protocolor) {
              float red = [protocolor red];
              float green = [protocolor green];
              float blue = [protocolor blue];
              FloatValue* alpha_wrapper = [protocolor alpha];
              float alpha = 1.0;
              if (alpha_wrapper != nil) {
                alpha = [alpha_wrapper value];
              }
              return [UIColor colorWithRed:red green:green blue:blue alpha:alpha];
           }

           static Color* toProto(UIColor* color) {
               CGFloat red, green, blue, alpha;
               if (![color getRed:&red green:&green blue:&blue alpha:&alpha]) {
                 return nil;
               }
               Color* result = [Color alloc] init];
               [result setRed:red];
               [result setGreen:green];
               [result setBlue:blue];
               if (alpha <= 0.9999) {
                 [result setAlpha:floatWrapperWithValue(alpha)];
               }
               [result autorelease];
               return result;
          }
          // ...

       Example (JavaScript):

          // ...

          var protoToCssColor = function(rgb_color) {
             var redFrac = rgb_color.red || 0.0;
             var greenFrac = rgb_color.green || 0.0;
             var blueFrac = rgb_color.blue || 0.0;
             var red = Math.floor(redFrac * 255);
             var green = Math.floor(greenFrac * 255);
             var blue = Math.floor(blueFrac * 255);

             if (!('alpha' in rgb_color)) {
                return rgbToCssColor_(red, green, blue);
             }

             var alphaFrac = rgb_color.alpha.value || 0.0;
             var rgbParams = [red, green, blue].join(',');
             return ['rgba(', rgbParams, ',', alphaFrac, ')'].join('');
          };

          var rgbToCssColor_ = function(red, green, blue) {
            var rgbNumber = new Number((red << 16) | (green << 8) | blue);
            var hexString = rgbNumber.toString(16);
            var missingZeros = 6 - hexString.length;
            var resultBuilder = ['#'];
            for (var i = 0; i < missingZeros; i++) {
               resultBuilder.push('0');
            }
            resultBuilder.push(hexString);
            return resultBuilder.join('');
          };

          // ...
    properties:
      alpha:
        description: |-
          The fraction of this color that should be applied to the pixel. That is,
          the final pixel color is defined by the equation:

            pixel color = alpha * (this color) + (1.0 - alpha) * (background color)

          This means that a value of 1.0 corresponds to a solid color, whereas
          a value of 0.0 corresponds to a completely transparent color. This
          uses a wrapper message rather than a simple float scalar so that it is
          possible to distinguish between a default value and the value being unset.
          If omitted, this color object is to be rendered as a solid color
          (as if the alpha value had been explicitly given with a value of 1.0).
        format: float
        type: number
      blue:
        description: 'The amount of blue in the color as a value in the interval [0, 1].'
        format: float
        type: number
      green:
        description: 'The amount of green in the color as a value in the interval [0, 1].'
        format: float
        type: number
      red:
        description: 'The amount of red in the color as a value in the interval [0, 1].'
        format: float
        type: number
    type: object
  ColorInfo:
    description: |-
      Color information consists of RGB channels, score and fraction of
      image the color occupies in the image.
    properties:
      color:
        $ref: '#/definitions/Color'
        description: RGB components of the color.
      pixelFraction:
        description: |-
          Stores the fraction of pixels the color occupies in the image.
          Value in range [0, 1].
        format: float
        type: number
      score:
        description: 'Image-specific score for this color. Value in range [0, 1].'
        format: float
        type: number
    type: object
  DominantColorsAnnotation:
    description: Set of dominant colors and their corresponding scores.
    properties:
      colors:
        description: 'RGB color values, with their score and pixel fraction.'
        items:
          $ref: '#/definitions/ColorInfo'
        type: array
    type: object
  EntityAnnotation:
    description: Set of detected entity features.
    properties:
      boundingPoly:
        $ref: '#/definitions/BoundingPoly'
        description: |-
          Image region to which this entity belongs. Not filled currently
          for `LABEL_DETECTION` features. For `TEXT_DETECTION` (OCR), `boundingPoly`s
          are produced for the entire text detected in an image region, followed by
          `boundingPoly`s for each word within the detected text.
      confidence:
        description: |-
          The accuracy of the entity detection in an image.
          For example, for an image containing 'Eiffel Tower,' this field represents
          the confidence that there is a tower in the query image. Range [0, 1].
        format: float
        type: number
      description:
        description: 'Entity textual description, expressed in its <code>locale</code> language.'
        type: string
      locale:
        description: |-
          The language code for the locale in which the entity textual
          <code>description</code> (next field) is expressed.
        type: string
      locations:
        description: |-
          The location information for the detected entity. Multiple
          <code>LocationInfo</code> elements can be present since one location may
          indicate the location of the scene in the query image, and another the
          location of the place where the query image was taken. Location information
          is usually present for landmarks.
        items:
          $ref: '#/definitions/LocationInfo'
        type: array
      mid:
        description: |-
          Opaque entity ID. Some IDs might be available in Knowledge Graph(KG).
          For more details on KG please see:
          https://developers.google.com/knowledge-graph/
        type: string
      properties:
        description: |-
          Some entities can have additional optional <code>Property</code> fields.
          For example a different kind of score or string that qualifies the entity.
        items:
          $ref: '#/definitions/Property'
        type: array
      score:
        description: 'Overall score of the result. Range [0, 1].'
        format: float
        type: number
      topicality:
        description: |-
          The relevancy of the ICA (Image Content Annotation) label to the
          image. For example, the relevancy of 'tower' to an image containing
          'Eiffel Tower' is likely higher than an image containing a distant towering
          building, though the confidence that there is a tower may be the same.
          Range [0, 1].
        format: float
        type: number
    type: object
  FaceAnnotation:
    description: A face annotation object contains the results of face detection.
    properties:
      angerLikelihood:
        description: Anger likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      blurredLikelihood:
        description: Blurred likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      boundingPoly:
        $ref: '#/definitions/BoundingPoly'
        description: |-
          The bounding polygon around the face. The coordinates of the bounding box
          are in the original image's scale, as returned in ImageParams.
          The bounding box is computed to "frame" the face in accordance with human
          expectations. It is based on the landmarker results.
          Note that one or more x and/or y coordinates may not be generated in the
          BoundingPoly (the polygon will be unbounded) if only a partial face appears in
          the image to be annotated.
      detectionConfidence:
        description: 'Detection confidence. Range [0, 1].'
        format: float
        type: number
      fdBoundingPoly:
        $ref: '#/definitions/BoundingPoly'
        description: |-
          This bounding polygon is tighter than the previous
          <code>boundingPoly</code>, and
          encloses only the skin part of the face. Typically, it is used to
          eliminate the face from any image analysis that detects the
          "amount of skin" visible in an image. It is not based on the
          landmarker results, only on the initial face detection, hence
          the <code>fd</code> (face detection) prefix.
      headwearLikelihood:
        description: Headwear likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      joyLikelihood:
        description: Joy likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      landmarkingConfidence:
        description: 'Face landmarking confidence. Range [0, 1].'
        format: float
        type: number
      landmarks:
        description: Detected face landmarks.
        items:
          $ref: '#/definitions/Landmark'
        type: array
      panAngle:
        description: |-
          Yaw angle. Indicates the leftward/rightward angle that the face is
          pointing, relative to the vertical plane perpendicular to the image. Range
          [-180,180].
        format: float
        type: number
      rollAngle:
        description: |-
          Roll angle. Indicates the amount of clockwise/anti-clockwise rotation of
          the
          face relative to the image vertical, about the axis perpendicular to the
          face. Range [-180,180].
        format: float
        type: number
      sorrowLikelihood:
        description: Sorrow likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      surpriseLikelihood:
        description: Surprise likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      tiltAngle:
        description: |-
          Pitch angle. Indicates the upwards/downwards angle that the face is
          pointing
          relative to the image's horizontal plane. Range [-180,180].
        format: float
        type: number
      underExposedLikelihood:
        description: Under-exposed likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
    type: object
  Feature:
    description: |-
      The <em>Feature</em> indicates what type of image detection task to perform.
      Users describe the type of Google Cloud Vision API tasks to perform over
      images by using <em>Feature</em>s. Features encode the Cloud Vision API
      vertical to operate on and the number of top-scoring results to return.
    properties:
      maxResults:
        description: Maximum number of results of this type.
        format: int32
        type: integer
      type:
        description: The feature type.
        enum:
          - TYPE_UNSPECIFIED
          - FACE_DETECTION
          - LANDMARK_DETECTION
          - LOGO_DETECTION
          - LABEL_DETECTION
          - TEXT_DETECTION
          - SAFE_SEARCH_DETECTION
          - IMAGE_PROPERTIES
        type: string
    type: object
  Image:
    description: Client image to perform Google Cloud Vision API tasks over.
    properties:
      content:
        description: 'Image content, represented as a stream of bytes.'
        format: byte
        type: string
      source:
        $ref: '#/definitions/ImageSource'
        description: |-
          Google Cloud Storage image location. If both 'content' and 'source'
          are filled for an image, 'content' takes precedence and it will be
          used for performing the image annotation request.
    type: object
  ImageContext:
    description: Image context.
    properties:
      languageHints:
        description: |-
          List of languages to use for TEXT_DETECTION. In most cases, an empty value
          will yield the best results as it will allow text detection to
          automatically detect the text language. For languages based on the latin
          alphabet a hint is not needed. In rare cases, when the language of
          the text in the image is known in advance, setting this hint will help get
          better results (although it will hurt a great deal if the hint is wrong).
          Text detection will return an error if one or more of the languages
          specified here are not supported. The exact list of supported languages are
          specified here:
          https://cloud.google.com/translate/v2/using_rest#language-params
        items:
          type: string
        type: array
      latLongRect:
        $ref: '#/definitions/LatLongRect'
        description: Lat/long rectangle that specifies the location of the image.
    type: object
  ImageProperties:
    description: Stores image properties (e.g. dominant colors).
    properties:
      dominantColors:
        $ref: '#/definitions/DominantColorsAnnotation'
        description: 'If present, dominant colors completed successfully.'
    type: object
  ImageSource:
    description: External image source (Google Cloud Storage image location).
    properties:
      gcsImageUri:
        description: |-
          Google Cloud Storage image URI. It must be in the following form:
          `gs://bucket_name/object_name`. For more
          details, please see: https://cloud.google.com/storage/docs/reference-uris.
          NOTE: Cloud Storage object versioning is not supported!
        type: string
    type: object
  Landmark:
    description: |-
      A face-specific landmark (for example, a face feature).
      Landmark positions may fall outside the bounds of the image
      when the face is near one or more edges of the image.
      Therefore it is NOT guaranteed that 0 <= x < width or 0 <= y < height.
    properties:
      position:
        $ref: '#/definitions/Position'
        description: Face landmark position.
      type:
        description: Face landmark type.
        enum:
          - UNKNOWN_LANDMARK
          - LEFT_EYE
          - RIGHT_EYE
          - LEFT_OF_LEFT_EYEBROW
          - RIGHT_OF_LEFT_EYEBROW
          - LEFT_OF_RIGHT_EYEBROW
          - RIGHT_OF_RIGHT_EYEBROW
          - MIDPOINT_BETWEEN_EYES
          - NOSE_TIP
          - UPPER_LIP
          - LOWER_LIP
          - MOUTH_LEFT
          - MOUTH_RIGHT
          - MOUTH_CENTER
          - NOSE_BOTTOM_RIGHT
          - NOSE_BOTTOM_LEFT
          - NOSE_BOTTOM_CENTER
          - LEFT_EYE_TOP_BOUNDARY
          - LEFT_EYE_RIGHT_CORNER
          - LEFT_EYE_BOTTOM_BOUNDARY
          - LEFT_EYE_LEFT_CORNER
          - RIGHT_EYE_TOP_BOUNDARY
          - RIGHT_EYE_RIGHT_CORNER
          - RIGHT_EYE_BOTTOM_BOUNDARY
          - RIGHT_EYE_LEFT_CORNER
          - LEFT_EYEBROW_UPPER_MIDPOINT
          - RIGHT_EYEBROW_UPPER_MIDPOINT
          - LEFT_EAR_TRAGION
          - RIGHT_EAR_TRAGION
          - LEFT_EYE_PUPIL
          - RIGHT_EYE_PUPIL
          - FOREHEAD_GLABELLA
          - CHIN_GNATHION
          - CHIN_LEFT_GONION
          - CHIN_RIGHT_GONION
        type: string
    type: object
  LatLng:
    description: |-
      An object representing a latitude/longitude pair. This is expressed as a pair
      of doubles representing degrees latitude and degrees longitude. Unless
      specified otherwise, this must conform to the
      <a href="http://www.unoosa.org/pdf/icg/2012/template/WGS_84.pdf">WGS84
      standard</a>. Values must be within normalized ranges.

      Example of normalization code in Python:

          def NormalizeLongitude(longitude):
            """Wraps decimal degrees longitude to [-180.0, 180.0]."""
            q, r = divmod(longitude, 360.0)
            if r > 180.0 or (r == 180.0 and q <= -1.0):
              return r - 360.0
            return r

          def NormalizeLatLng(latitude, longitude):
            """Wraps decimal degrees latitude and longitude to
            [-180.0, 180.0] and [-90.0, 90.0], respectively."""
            r = latitude % 360.0
            if r <= 90.0:
              return r, NormalizeLongitude(longitude)
            elif r >= 270.0:
              return r - 360, NormalizeLongitude(longitude)
            else:
              return 180 - r, NormalizeLongitude(longitude + 180.0)

          assert 180.0 == NormalizeLongitude(180.0)
          assert -180.0 == NormalizeLongitude(-180.0)
          assert -179.0 == NormalizeLongitude(181.0)
          assert (0.0, 0.0) == NormalizeLatLng(360.0, 0.0)
          assert (0.0, 0.0) == NormalizeLatLng(-360.0, 0.0)
          assert (85.0, 180.0) == NormalizeLatLng(95.0, 0.0)
          assert (-85.0, -170.0) == NormalizeLatLng(-95.0, 10.0)
          assert (90.0, 10.0) == NormalizeLatLng(90.0, 10.0)
          assert (-90.0, -10.0) == NormalizeLatLng(-90.0, -10.0)
          assert (0.0, -170.0) == NormalizeLatLng(-180.0, 10.0)
          assert (0.0, -170.0) == NormalizeLatLng(180.0, 10.0)
          assert (-90.0, 10.0) == NormalizeLatLng(270.0, 10.0)
          assert (90.0, 10.0) == NormalizeLatLng(-270.0, 10.0)
    properties:
      latitude:
        description: 'The latitude in degrees. It must be in the range [-90.0, +90.0].'
        format: double
        type: number
      longitude:
        description: 'The longitude in degrees. It must be in the range [-180.0, +180.0].'
        format: double
        type: number
    type: object
  LatLongRect:
    description: Rectangle determined by min and max LatLng pairs.
    properties:
      maxLatLng:
        $ref: '#/definitions/LatLng'
        description: Max lat/long pair.
      minLatLng:
        $ref: '#/definitions/LatLng'
        description: Min lat/long pair.
    type: object
  LocationInfo:
    description: Detected entity location information.
    properties:
      latLng:
        $ref: '#/definitions/LatLng'
        description: Lat - long location coordinates.
    type: object
  Position:
    description: |-
      A 3D position in the image, used primarily for Face detection landmarks.
      A valid Position must have both x and y coordinates.
      The position coordinates are in the same scale as the original image.
    properties:
      x:
        description: X coordinate.
        format: float
        type: number
      'y':
        description: Y coordinate.
        format: float
        type: number
      z:
        description: Z coordinate (or depth).
        format: float
        type: number
    type: object
  Property:
    description: Arbitrary name/value pair.
    properties:
      name:
        description: Name of the property.
        type: string
      value:
        description: Value of the property.
        type: string
    type: object
  SafeSearchAnnotation:
    description: |-
      Set of features pertaining to the image, computed by various computer vision
      methods over safe-search verticals (for example, adult, spoof, medical,
      violence).
    properties:
      adult:
        description: Represents the adult contents likelihood for the image.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      medical:
        description: Likelihood this is a medical image.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      spoof:
        description: |-
          Spoof likelihood. The likelihood that an obvious modification
          was made to the image's canonical version to make it appear
          funny or offensive.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      violence:
        description: Violence likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
    type: object
  Status:
    description: |-
      The `Status` type defines a logical error model that is suitable for different
      programming environments, including REST APIs and RPC APIs. It is used by
      [gRPC](https://github.com/grpc). The error model is designed to be:

      - Simple to use and understand for most users
      - Flexible enough to meet unexpected needs

      # Overview

      The `Status` message contains three pieces of data: error code, error message,
      and error details. The error code should be an enum value of
      google.rpc.Code, but it may accept additional error codes if needed.  The
      error message should be a developer-facing English message that helps
      developers *understand* and *resolve* the error. If a localized user-facing
      error message is needed, put the localized message in the error details or
      localize it in the client. The optional error details may contain arbitrary
      information about the error. There is a predefined set of error detail types
      in the package `google.rpc` which can be used for common error conditions.

      # Language mapping

      The `Status` message is the logical representation of the error model, but it
      is not necessarily the actual wire format. When the `Status` message is
      exposed in different client libraries and different wire protocols, it can be
      mapped differently. For example, it will likely be mapped to some exceptions
      in Java, but more likely mapped to some error codes in C.

      # Other uses

      The error model and the `Status` message can be used in a variety of
      environments, either with or without APIs, to provide a
      consistent developer experience across different environments.

      Example uses of this error model include:

      - Partial errors. If a service needs to return partial errors to the client,
          it may embed the `Status` in the normal response to indicate the partial
          errors.

      - Workflow errors. A typical workflow has multiple steps. Each step may
          have a `Status` message for error reporting purpose.

      - Batch operations. If a client uses batch request and batch response, the
          `Status` message should be used directly inside batch response, one for
          each error sub-response.

      - Asynchronous operations. If an API call embeds asynchronous operation
          results in its response, the status of those operations should be
          represented directly using the `Status` message.

      - Logging. If some API errors are stored in logs, the message `Status` could
          be used directly after any stripping needed for security/privacy reasons.
    properties:
      code:
        description: 'The status code, which should be an enum value of google.rpc.Code.'
        format: int32
        type: integer
      details:
        description: |-
          A list of messages that carry the error details.  There will be a
          common set of message types for APIs to use.
        items:
          additionalProperties:
            description: Properties of the object. Contains field @ype with type URL.
          type: object
        type: array
      message:
        description: |-
          A developer-facing error message, which should be in English. Any
          user-facing error message should be localized and sent in the
          google.rpc.Status.details field, or localized by the client.
        type: string
    type: object
  Vertex:
    description: |-
      A vertex represents a 2D point in the image.
      NOTE: the vertex coordinates are in the same scale as the original image.
    properties:
      x:
        description: X coordinate.
        format: int32
        type: integer
      'y':
        description: Y coordinate.
        format: int32
        type: integer
    type: object
